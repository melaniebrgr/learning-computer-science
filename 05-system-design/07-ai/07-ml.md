# Machine learning (ML)

ML is a subset of AI. In typical software development we craft algorithms that take data and rules as inputs and produce answers. In ML, we use the answers and the data to extract the rules. This is for cases where the rules are complex or difficult to define explicitly. For example, in image recognition of animals, and customer behavior prediction.

To restate, instead of crafting the rules for a specific task, you train a model by providing it with examples (answers). The "answers" can be labeled examples (supervised learning), no labels (unsupervised learning), a mix of both (semi-supervised learning), and reinforcement learning.

ML techniques are either descriptive or predictive:

- anomaly detection for identifying unusual cases like fraud,
- classification for categorizing new data,
- regression for predicting continuous values, and
- clustering for grouping similar data points without labels.

## Lifecycle

A problem statement is first clearly defined.

1. **Data collection**: Gather relevant data from various sources, such as databases, APIs, or user inputs.
2. **Data preparation**: Clean and transform the data to make it suitable for training. Includes cleaning, removing outliers, removing missing values, proper data type formatting, calculating new values. About 80% of the data set should be reserved for training, and 20% for testing or evaluation.
3. **Continuous monitoring**: Monitor the model's performance over time to ensure it continues to meet the desired accuracy and reliability standards. This may involve retraining the model with new data or adjusting its parameters as needed.

## Tools

- **Matplotlib**: customizable plots
- **scikit-learn**: building traditional machine learning models
- **Pandas**: data analysis and preparation
- **NumPy**: numerical computations
- **SciPy**: scientific computing
- **TensorFlow, Keras, Theano, and PyTorch**: deep learning frameworks

## Regression

Use regression when you want to predict a continuous value, e.g. sales forecasting, price estimation, employment income, predictive maintenance. There are many regression algorithms that are suited to different contexts and conditions, e.g. linear, polynomial, random forest, extreme gradient boosting, K-nearest neighbours.

### Simple linear regression

A single, independent variable is used to predict a dependent variable. The simple regression can be linear or non-linear. A linear regression imposes a linear relationship between the independent and dependent variables. Non-linear regression can be polynomial, logarithmic, exponential, or power regression or any curve that fits the data.

In simple linear regression, the relationship between the independent variable (X) and the dependent variable (Y) is represented as: y = mx + b. The goal is to find the best-fitting line that minimizes the distance between the predicted values and the actual data points. MSE (Mean Squared Error) measures how poorly a regression line fits the values of the dataset. The method of finding the best-fitting line by minimizing the MSE is called the OLS (Ordinary Least Squares) is a method.

#### Model preparation

1. Collect the data: load a CSV file with data and plot and explore it, does is appears to have a linear relationship?
2. Extract the input features, e.g. x and y values.
3. Split the data set into training and testing sets, e.g. 80% for training and 20% for testing.
4. Train the model by fitting a line of regression. Plot the line of regression over the data. Does it look like a good fit?
5. Evaluate the model by calculating the **Mean absolute error**, **Mean squared error**, **Root mean squared error**, and **R-squared**, and making test predictions.

```python
# Simple linear regression example

# Install the required packages if not already installed
# !pip install numpy==2.2.0
# !pip install pandas==2.2.3
# !pip install scikit-learn==1.6.0
# !pip install matplotlib==3.9.3

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline

url= "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv"
df=pd.read_csv(url)
df.sample(5)

cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
cdf.sample(5)

viz = cdf[['CYLINDERS','ENGINESIZE','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
viz.hist()
plt.show()

plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,  color='blue')
plt.xlabel("FUELCONSUMPTION_COMB")
plt.ylabel("Emission")
plt.show()

X = cdf.ENGINESIZE.to_numpy()
y = cdf.CO2EMISSIONS.to_numpy()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

from sklearn import linear_model

# create a model object
regressor = linear_model.LinearRegression()

# train the model on the training data
# X_train is a 1-D array but sklearn models expect a 2D array as input for the training data, with shape (n_observations, n_features).
# So we need to reshape it. We can let it infer the number of observations using '-1'.
regressor.fit(X_train.reshape(-1, 1), y_train)

# Print the coefficients
print ('Coefficients: ', regressor.coef_[0]) # with simple linear regression there is only one coefficient, here we extract it from the 1 by 1 array.
print ('Intercept: ',regressor.intercept_)

plt.scatter(X_train, y_train,  color='blue')
plt.plot(X_train, regressor.coef_ * X_train + regressor.intercept_, '-r')
plt.xlabel("Engine size")
plt.ylabel("Emission")

```

#### Model evaluation

The actual values and predicted values are compared to calculate the accuracy of a regression model. Evaluation metrics play a key role in the development of a model, as they provide insight into areas that require improvement. There are different model evaluation metrics, let's use MSE here to calculate the accuracy of our model based on the test set:

1. Mean Absolute Error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since itâ€™s just an average error.
2. Mean Squared Error (MSE): MSE is the mean of the squared error. In fact, it's the metric used by the model to find the best fit line, and for that reason, it is also called the residual sum of squares.
3. Root Mean Squared Error (RMSE). RMSE simply transforms the MSE into the same units as the variables being compared, which can make it easier to interpret.
4. R-squared is not an error but rather a popular metric used to estimate the performance of your regression model. It represents how close the data points are to the fitted regression line. The higher the R-squared value, the better the model fits your data. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).

### Multiple linear regression (MLR)

Multiple linear regression is when more than one independent variable is used to predict the value of a dependent variable. The regression equation will indicate the strength of each independent variable on the dependent variable.

While multiple linear regression provides a better model, too many variables can lead to overfitting and poor generalization to new data. The model will also become too complex, and can capture noise instead of the underlying trend. Techniques like regularization (e.g., Lasso, Ridge) can help mitigate this issue by penalizing overly complex models.

The main applications of MLR are predicting outcomes, and predicting the impact of changes to one of the independent variables.

While a simple regression is graphed on a 2D plane, multiple regression is graphed in 3D space. For example, for the simplest multiple regression when there are two independent variables, `y = b0 + b1*x1 + b2*x2`, is a plane in 3D space.

### Non-linear regression

Where the relationship between the independent and dependent variables is not linear, non-linear regression is used. Non-linear regression can be polynomial, logarithmic, exponential, or power regression or any curve that fits the data.
