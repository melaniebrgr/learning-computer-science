# AI

Artificial intelligence, or AI, makes computers appear intelligent by simulating the cognitive abilities of humans. AI is a general field with a broad scope. It comprises computer vision, natural language processing, generative AI, machine learning, and deep learning.

An AI engineer is an "AI systems builder" typically using AI in a generative modality (though sometimes also for decisions and recommendations). Unlike data scientists, unstructured data of billions to trillions of tokens are used to train models. Application scope tends to be broader, require more computation, and longer training times. Development lifecycle starts with a use case, model selection, prompt engineering and embedding into an application. Application types include chaining, PEFT, RAG, and Agents.

Artificial intelligence (AI) simulates human cognition, while machine learning (ML) uses algorithms and requires feature engineering to learn from data.

## Definitions

- **large language model (LLM)**: A mathematical model trained on vast amounts of text data that can be used to predict what comes next for any piece of text. All possible next "words" are assigned a probability. Training involves adjusting the model's probabilities of the next selected words to minimize the difference between predicted and actual next words in the training data. The mathematical model can have hundreds of billions of parameters.

- **narrow AI**: AI that is designed to perform a specific task, typically a binary classifier, such as language translation, product recommendation, or fraud detection. It is not capable of generalizing its knowledge to other tasks.

- **machine learning**: A subset of AI that involves training algorithms to learn patterns from data and make predictions or decisions based on that data, e.g. predicting what a user will click on next, predicting banking and loan risk. Includes neural networks, deep and shallow. The data can be text or statistical.

- **deep learning**: A subset of machine learning that uses neural networks with many layers to learn complex patterns in large datasets, e.g. image and speech recognition.

- **natural language processing (NLP)**: A field of AI that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language. **neural linguistic AI**: Text and meaning in text. LLMs replacing NLP tasks.

- **computer vision**: A field of AI that enables machines to interpret and understand visual information from the world, such as images and videos, allowing them to recognize objects, faces, and scenes, and enabling auto-tagging of photos, for example.

- **expert system**: A computer system that uses knowledge and inference rules to solve specific problems within a particular domain, such as medical diagnosis or legal reasoning. Rules are often encoded in a knowledge base.

- **supervised learning**: A type of machine learning where the model is trained on labeled data, meaning the input data is paired with the correct output, allowing the model to learn the mapping between inputs and outputs.

- **unsupervised learning**: A type of machine learning where the model is trained on unlabeled data, meaning the input data does not have corresponding output labels, allowing the model to discover patterns and relationships in the data.

- **semi-supervised learning**: A type of machine learning that combines both labeled and unlabeled data for training, allowing the model to learn from a small amount of labeled data while leveraging the larger amount of unlabeled data.

- **reinforcement learning**: A type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward signal, often used in robotics and game playing.

## Models

A model is mapping of input data X to output data Y. A modern ML “model” is best thought of as a learned mathematical function, `f0`, that maps inputs to outputs, where 0 is a big collection of learned parameters (weights and biases). In deep learning, those parameters are stored as huge tensors of numbers, so you can think of the model both as “a function” and as “a structured pile of weights” that define that function.

Practically, when people talk about “loading a model,” they usually mean loading its weights file(s), because the architecture is fixed by code and the weights encode what it has learned. Conceptually, though, the model is the entire parameterized function—architecture plus parameters—not only a bare sequence of weights and not just a single closed‑form equation

"Frontier models" are usually very large, general‑purpose foundation models that can handle many tasks (reasoning, coding, multimodal input) and often show new, emergent behaviors as they scale.

## Large language models (LLMs)

Large language models (LLMs) are a type of AI that can understand and generate human language. They are trained on vast amounts of text data and can perform various tasks, such as translation, summarization, question answering, and more. LLMs are often used in applications like chatbots, virtual assistants, and content generation.

Before LLMs appeared in 2022 we had different specific tools for summarization, and translation. LLMs are a single but multi-purpose tool. Now we are asking what else they can do. Write a novel? Make a scientific discovery?

Here is an updated table including DeepSeek, Grok, and Cohere’s frontier model line.

| Vendor    | Model | Max context window | Notable capabilities | Ideal use |
|---|---|---|---|---|
| OpenAI   | GPT‑5 class  | ~256k tokens | Strong general reasoning, advanced code generation and refactoring, robust tool use and “agentic” workflows on large projects. | Complex coding, orchestration of tools and agents, and research workflows over large repos or multi‑doc workspaces. |
| Anthropic | Claude Sonnet 4.x / Opus 4.x | ~1M tokens on enterprise tiers, ~200k on standard | Very strong long‑context reading and synthesis, safe dialogue and coding, designed for large‑scale document and knowledge work. | Legal / policy review, multi‑document synthesis, enterprise knowledge assistants, long‑horizon agents over huge text corpora. |
| Google | Gemini 1.5 family | ~1M tokens in long‑context configs | Multimodal frontier model (text, code, images, audio, video) with strong long‑context reasoning across modalities. | Multimodal workflows like analyzing documents plus images/video, product or UX flows that blend many data types. |
| DeepSeek | DeepSeek‑V3.x | 128k–164k tokens depending on version | Open frontier‑class model with strong reasoning and code performance, cost‑efficient long‑context and “thinking/non‑thinking” modes.| Lower‑cost frontier‑style reasoning and coding, especially for open‑source or self‑hosted setups needing substantial but not 1M‑token context. |
| xAI | Grok 4.x | ~256k tokens, with reports of up to ~2M in certain “Fast” modes | Real‑time integration with X/Twitter data, long‑context conversational reasoning, content and code generation with live retrieval. | Chat and agentic tools that need live information from X, long conversational threads, and content creation grounded in current streams. |

### Aspects of LLMs

- **Reasoning**, a.k.a "true abstract reasoning": The original "LLM boom" was more related to what’s called “emerging capabilities”. The reasoning trend started later. It was also related to solving math and programming tasks.

- **Agents**: LLMs orchestrate to decide which strategies to use. Google had a paper that demonstrated a research assistant multi-agentive tool.

- **Open-endedness**: It has the potential to adapt to solve many different tasks. You don't need to tune it to play a particular game. It can learn.

- **Fine-tuning**: An example of this process involves taking a model, like one on open AI, uploading a JSONL file, setting up prompts with the desired and not desired output (the kinds of answers it should give), and running the fine-tuning process. It adjust the wieghts of the model until it fits the outcome you wish. This is best for specific tasks or domains, like customer support or legal advice. Medium effort, but can be expensive, e.g. 100 EUR per fine-tuning run. The end result is that you can train models to be more accurate at specific tasks, such as generating code, having a specific personality, or generating images in a specific style.

- **retrieval augmented generation (RAG)**: An example of NotebookLM, which is a retrieval augmented generation model. It can access external data sources to provide more accurate and up-to-date information. Very powerful but expensive (20 EUR month per user), deploy DB, search engine, plus compute to convert text in vectors.

- **model context protocol (MCP)**: An example of this is giving GitHub Copilot permission to access APIs and do additional things, like post to Slack, or create a pull request.

- **context window**: AI language model APIs are not stateful. There are no persistent sessions or cookies. When sending a message, the entire chat history is sent as if the model has never seen any previous messages. A context window is the limited amount of text/tokens that an AI model can process and remember during an interaction including the input and output. The size of the context window can vary between models and impacts the model's performance and complexity. Managing the context window becomes crucial for effective interactions.

### Applications of LLMs

- **ChatGPT**: Very good if you want something quick, but usage gets quickly repetitive. The file is fixed, and static and gets out of date.
- **NotebookLM**: Was the first to demo RAG, it references the text in the source. Works well for FAQs, documentation. It is sort of up to date, because you can manually update the source and resume the session.

### Ways of using LLMs

1. manual/UI: You interact with the LLM through a user interface, like ChatGPT. This is best for personal use. Low effort. Dump in a file attach it, and start prompting.
2. API: You use the LLM through an API, like OpenAI's API. This is best for integrating LLMs into applications or services. Medium effort. A good sweet spot for startups.
3. Cloud infrastructure: You deploy the LLM on your own infrastructure or use a cloud provider, like using Hugging Face's Transformers library. This is best for large-scale applications or when you need more control over the model. High effort. Google is quite rate limited. AWS is good since it offers access to anthropic.

TL;DR Calling APIs are not the only way to do things. Some things have privacy limitations so can't just make API calls or can only call to entities in the same geographical space.

### Impact of LLM size

The size of an LLM impacts its performance and capabilities. Larger models tend to have more parameters, which allows them to learn more complex patterns and relationships in the data. However, they require more computational resources and memory, and are more expensive to train and deploy and have much greater environmental impacts. By contrast smaller models are faster, cheaper but may not perform as well on certain tasks.

- **edge AI**: Something that Cloudflare is pushing for. It involves running AI algorithms directly on devices at the "edge" of a network, rather than relying on cloud servers. This means processing data locally, close to the user. The typical budge is 100-1GB
- **tiny AI**: A term used to describe small, efficient AI models that can run on low-power devices, such as smartphones, IoT devices or in browser extensions. The typical budget is 1-100MB.
- **nano AI**: A term used to describe extremely small AI models that can run on very low-power devices, such as microcontrollers or embedded systems. The typical budget is <1MB.

With smaller modesl you can build privacy friendly or offline applications. You can also have a hybrid application that only callas external APIS with extra horsepower is necessary. Applications of small AI models include:

- image classification
- speech recognition
- text classification
- anomaly detection
- sentiment analysis

Divide LLM usage into “cheap” and “smart” use cases and use models accordingly.

- Cheap: project naming, chat naming, and follow up questions are just basic prompts using gpt-4o-mini.
- Smart: full-file coding.

### LLM Project pipeline

1. LLM Dataset Generation
1. PyTorch Training (Python)
1. Weight Quantization (Int8)
1. TypeScript Code Generation
1. Browser Extension (WXT)
1. Web Worker Inference

## Resources

- OpenRouter.ai LLMs rankings: <https://openrouter.ai/rankings>
- LLM Tools: <https://ossinsight.io/collections/llm-tools/>
- LLM Dev Tools: <https://ossinsight.io/collections/llm-dev-tools/>
- Vector Databases: <https://ossinsight.io/collections/vector-search-engine/>
- Articial Intelligence: <https://ossinsight.io/collections/artificial-intelligence/>
- Machine Learning in Rust: <https://ossinsight.io/collections/ml-in-rust>
- ChatGPT Apps: <https://ossinsight.io/collections/chat-gpt-apps/>
- ChatGPT Alternatives: <https://ossinsight.io/collections/chat-gpt-alternatives/>
- Machine Learning OPs Tools: <https://ossinsight.io/collections/ml-ops-tools/>
- Best GPUs latest rankings/benchmarkings & average price (for LLMs usage): <https://benchmarks.ul.com/compare/best-gpus>
- Building Smarter Web Apps with LLMs: <https://www.youtube.com/watch?v=Nau7VIZfV5c> (prompting)
